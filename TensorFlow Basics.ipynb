{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In mathematics, tensors are geometric objects that describe linear relations between geometric vectors, scalars, and other tensors. Elementary examples of such relations include the dot product, the cross product, and linear maps. Geometric vectors, often used in physics and engineering applications, and scalars themselves are also tensors [Reference: Wiki].\n",
    "\n",
    "TensorFlow is an open source library for numerical computation, specializing in machine learning applications. [Refer to this paper for in-depth detail: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45166.pdf]\n",
    "\n",
    "A  TensorFlow  computation  is  described  by  a  directed graph, which is composed of a set of nodes. \n",
    "The graph represents a dataflow computation,  with extensions for allowing  some  kinds  of  nodes  to  maintain  and  update persistent  state  and  for  branching  and  looping  control structures within the graph.\n",
    "\n",
    "In a TensorFlow graph, each node has zero or more inputs and zero or more outputs, and represents the instantiation of an operation.\n",
    "\n",
    "Values that flow along normal edges in the graph (from outputs to inputs) are tensors, arbitrary dimensionality arrays where the underlying element type is specified or inferred at graph-construction time. \n",
    "\n",
    "<img src=\"assets/add.png\" style=\"height: 50%;width: 50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special  edges,  called control  dependencies ,  can also exist in the graph:  no data flows along such edges, but they indicate that the source node for the control dependence  must  finish  executing  before  the  destination\n",
    "node for the control dependence starts executing. This is done using *Topological sort*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define your network, you'll need to define the order of operations for your nodes. Given that the input to some node depends on the outputs of others, you need to flatten the graph in such a way where all the input dependencies for each node are resolved before trying to run its calculation. This is a technique called a topological sort.\n",
    "\n",
    "In the field of computer science, a topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge $u \\rightarrow v$ from vertex $u$ to vertex $v$, $u$ comes before $v$ in the ordering.\n",
    "\n",
    "<img src=\"assets/topological_sort.png\" style=\"height: 50%;width: 50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.Input object at 0x000001DDCE17F160>, <__main__.Input object at 0x000001DDCE17F358>] [<__main__.Multiply object at 0x000001DDCE17F240>]\n",
      "(10 + 5)*10 = 150 (according to flow)\n"
     ]
    }
   ],
   "source": [
    "# Let first define the basis on which TensorFlow is build\n",
    "# Lets do a simple Add operation\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Nodes from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Nodes to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # Add this node as an outbound node on its inputs.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "# Input is a subclass, which has the inheritence from the parent class (Node)\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # an Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "    # NOTE: Input node is the only node that may\n",
    "    # receive its value as an argument to forward().\n",
    "    #\n",
    "    # All other node implementations should calculate their\n",
    "    # values from the value of previous nodes, using\n",
    "    # self.inbound_nodes\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "\n",
    "# This is Add subclass\n",
    "class Add(Node):\n",
    "    def __init__(self, x, y):\n",
    "        # You could access `x` and `y` in forward with\n",
    "        # self.inbound_nodes[0] (`x`) and self.inbound_nodes[1] (`y`)\n",
    "        Node.__init__(self, [x, y])\n",
    "        # print (self.inbound_nodes)\n",
    "        # print \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node (`self.value`) to the sum of its inbound_nodes.\n",
    "        \"\"\"\n",
    "        self.value=self.inbound_nodes[0].value+self.inbound_nodes[1].value\n",
    "\n",
    "class Multiply(Node):\n",
    "    def __init__(self, x, y):\n",
    "        Node.__init__(self, [x, y]) # Initialize the mulitply node with it's in-bound nodes\n",
    "    \n",
    "    def forward(self):\n",
    "        self.value = self.inbound_nodes[0].value * self.inbound_nodes[1].value\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n",
    "\n",
    "x, y = Input(), Input() # Inputs does not have any in-bound nodes\n",
    "#print (x.outbound_nodes)\n",
    "#print (y.outbound_nodes)\n",
    "\n",
    "f = Add(x, y) # Once an Add subclass is called, it updates an Add (node) as outbound node to all its inbound nodes (which are x and y)\n",
    "#print (f.inbound_nodes, f.outbound_nodes)\n",
    "\n",
    "g = Multiply(f, x)\n",
    "# print (f.inbound_nodes, f.outbound_nodes)\n",
    "#print (x.outbound_nodes)\n",
    "#print (f.value)\n",
    "\n",
    "\n",
    "feed_dict = {x: 10, y: 5} # here x and y are input nodes\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "# print (sorted_nodes)\n",
    "output = forward_pass(f, sorted_nodes)\n",
    "# output = forward_pass(g, sorted_nodes)\n",
    "\n",
    "# NOTE: because topological_sort set the values for the `Input` nodes we could also access\n",
    "# the value for x with x.value (same goes for y).\n",
    "print(\"{} + {} = {} (according to flow)\".format(feed_dict[x], feed_dict[y], output))\n",
    "# print(\"({} + {})*{} = {} (according to flow)\".format(feed_dict[x], feed_dict[y], feed_dict[x], output))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"linear_combination.png\" style=\"height: 50%;width: 50%\">\n",
    "\n",
    "In a TensorFlow world, this looks like:\n",
    "<img src=\"assets/linear_tensor.png\" style=\"height: 50%;width: 50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Linear(Node):\n",
    "    def __init__(self, inputs, weights, bias):\n",
    "        Node.__init__(self, [inputs, weights, bias])\n",
    "    \n",
    "    def forward(self):\n",
    "        # Check if the inbound instances are numpy arrays\n",
    "        if isinstance(self.inbound_nodes[0].value, np.ndarray) is False:\n",
    "            self.inbound_nodes[0].value = np.array(self.inbound_nodes[0].value)\n",
    "        if isinstance(self.inbound_nodes[1].value, np.ndarray) is False:\n",
    "            self.inbound_nodes[1].value = np.array(self.inbound_nodes[1].value)\n",
    "        if isinstance(self.inbound_nodes[2].value, np.ndarray) is False:\n",
    "            self.inbound_nodes[2].value = np.array(self.inbound_nodes[2].value)\n",
    "        # You can also write assertions to make sure the dimensions match\n",
    "        self.value = np.dot(self.inbound_nodes[0].value, self.inbound_nodes[1].value) + self.inbound_nodes[2].value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 13.5  14.5]\n",
      " [ 18.   22. ]\n",
      " [ 35.   36. ]]\n"
     ]
    }
   ],
   "source": [
    "# Let see if we have implemented the Linear node correctly\n",
    "# First define the input objects\n",
    "X = Input()\n",
    "W = Input()\n",
    "b = Input()\n",
    "\n",
    "# Now define the Linear node\n",
    "Y = Linear(X, W, b)\n",
    "\n",
    "# Now feed these nodes some input values\n",
    "feed_dict = {X: np.array([[2, 3], [1, 5], [6, 7]]), W: np.array([[2, 1], [3, 4]]),  b: np.array([[0.5], [1.0], [2.0]])}\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "output = forward_pass(Y, sorted_nodes)\n",
    "\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's build a sigmoid node\n",
    "class Sigmoid(Node):\n",
    "    def __init__(self, inputs):\n",
    "        Node.__init__(self, [inputs]) # inbound nodes to the node object should be iterable\n",
    "    \n",
    "    def forward(self):\n",
    "        # print (self.inbound_nodes[0].value)\n",
    "        self.value = 1./(1 + np.exp(-self.inbound_nodes[0].value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99999863  0.9999995 ]\n",
      " [ 0.99999998  1.        ]\n",
      " [ 1.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "Y_sigmoid = Sigmoid(Y)\n",
    "# Now feed these nodes some input values\n",
    "feed_dict = {X: np.array([[2, 3], [1, 5], [6, 7]]), W: np.array([[2, 1], [3, 4]]),  b: np.array([[0.5], [1.0], [2.0]])}\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "output = forward_pass(Y_sigmoid, sorted_nodes)\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's implement the cost function, which is the mean-square-error\n",
    "class MSE(Node):\n",
    "    def __init__(self, y_true, y_hat):\n",
    "        Node.__init__(self, [y_true, y_hat])\n",
    "    \n",
    "    def forward(self):\n",
    "        \n",
    "        self.value = np.sum((self.inbound_nodes[0].value - \n",
    "                            self.inbound_nodes[1].value)**2)*1./self.inbound_nodes[0].value.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.416666666666668"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now test the cost function\n",
    "\n",
    "y_true = Input() # This is another input node to the MSE\n",
    "y_hat = Input()\n",
    "\n",
    "cost = MSE(y_true, y_hat)\n",
    "y_ = np.array([1, 2, 3])\n",
    "a_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y_true: y_, y_hat: a_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "forward_pass(cost, graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation in neural network\n",
    "\n",
    "<img src=\"assets/forward_propagation_nn.png\" style=\"height: 75%;width: 75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation in Neural Networks\n",
    "\n",
    "From the above figure we see that change in $L_2$ produces change in $C$. We call this relationship as the **Gradient**. Mathematically it is defined as:\n",
    "$$\\frac{\\partial C}{\\partial l_2}$$\n",
    "\n",
    "If we want to update one of the weights with gradient descent, we'll need the gradient of the cost with respect to those weights. Let's see how we can use this framework to find the gradient for the weights in the second layer, $W_2$. We want to calculate the gradient of $C$ with respect to $W_2$:\n",
    "$$\\frac{\\partial C}{\\partial w_2}$$\n",
    "\n",
    "The mean-square error which is the cost function, is given by:\n",
    "$$C = \\frac{1}{2m} \\Sigma_x[y_{true} - \\hat{y}]^{2}$$\n",
    "$$C = \\frac{1}{2m} \\Sigma_x[y_{true} - L_2]^{2}$$\n",
    "\n",
    "Where $L_2$ is the output from the Layer-2. So the gradient for $L_2$ node is:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial L_2} = \\frac{\\partial}{\\partial L_2} [\\frac{1}{2m}\\Sigma_x (y_{true}-L_2)^2]$$\n",
    "\n",
    "$$ = \\frac{-1}{m}[\\Sigma_x (y_{true}-L_2)]$$\n",
    "\n",
    "Now let's look at changes in $L_2$ due to the changes in $W_2$:\n",
    "$$\\frac{\\partial L_w}{\\partial W_2} = \\frac{\\partial}{\\partial W_2}[W_2\\times s_1+b_2]$$\n",
    "$$ = s_1 $$ with the size $(m, n_1)$\n",
    "\n",
    "Thus by chain-rule:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W_2} = \\frac{\\partial C}{\\partial L_2}\\times\\frac{\\partial L_2}{\\partial W_2}$$\n",
    "\n",
    "Thus the change in the weights for the second hidden-layer is the:\n",
    "\n",
    "$$\\delta W_2  = -s_2\\times \\frac{1}{m}  \\Sigma(y_{true}-L_2) $$\n",
    "\n",
    "\n",
    "<img src=\"assets/back_propagation_1.png.jpg\" style=\"height: 75%;width: 75%\"</img>\n",
    "\n",
    "<img src=\"assets/back_propagation_2.jpg\" style=\"height: 75%;width: 75%\"</img>\n",
    "\n",
    "In code this is given by:\n",
    "<prev>\n",
    "```# In Code this is given by\n",
    "# Initialize a partial for each of the inbound_nodes.\n",
    "self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "# Cycle through the outputs. The gradient will change depending\n",
    "# on each output, so the gradients are summed over all outputs.\n",
    "for n in self.outbound_nodes:\n",
    "    # Get the partial of the cost with respect to this node.\n",
    "    grad_cost = n.gradients[self]\n",
    "    # Set the partial of the loss with respect to this node's inputs.\n",
    "    self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "    # Set the partial of the loss with respect to this node's weights.\n",
    "    self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "    # Set the partial of the loss with respect to this node's bias.\n",
    "    self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "```\n",
    "</prev>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
      "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
      "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
      "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_nodes:\n",
    "            self.gradients[self] += n.gradients[self]\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Sum the partial with respect to the input over all the outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    \"\"\"\n",
    "    Updates the value of each trainable with SGD.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `trainables`: A list of `Input` Nodes representing weights/biases.\n",
    "        `learning_rate`: The learning rate.\n",
    "    \"\"\"\n",
    "    # TODO: update all the `trainables` with SGD\n",
    "    # You can access and assign the value of a trainable with `value` attribute.\n",
    "    # Example:\n",
    "    for t in trainables:\n",
    "        t.value = t.value - learning_rate*t.gradients[t]\n",
    "\n",
    "\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "y = Input()\n",
    "f = Linear(X, W, b)\n",
    "a = Sigmoid(f)\n",
    "cost = MSE(y, a)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2.], [3.]])\n",
    "b_ = np.array([-3.])\n",
    "y_ = np.array([1, 2])\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W: W_,\n",
    "    b: b_,\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_and_backward(graph)\n",
    "# return the gradients for each Input\n",
    "gradients = [t.gradients[t] for t in [X, y, W, b]]\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
    "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
    "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
    "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n",
    "\"\"\"\n",
    "print(gradients)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 120.109\n",
      "Epoch: 2, Loss: 33.018\n",
      "Epoch: 3, Loss: 27.542\n",
      "Epoch: 4, Loss: 23.355\n",
      "Epoch: 5, Loss: 24.365\n",
      "Epoch: 6, Loss: 15.663\n",
      "Epoch: 7, Loss: 16.942\n",
      "Epoch: 8, Loss: 14.217\n",
      "Epoch: 9, Loss: 15.675\n",
      "Epoch: 10, Loss: 18.921\n",
      "Epoch: 11, Loss: 11.797\n",
      "Epoch: 12, Loss: 16.114\n",
      "Epoch: 13, Loss: 14.660\n",
      "Epoch: 14, Loss: 16.879\n",
      "Epoch: 15, Loss: 15.137\n",
      "Epoch: 16, Loss: 14.133\n",
      "Epoch: 17, Loss: 14.237\n",
      "Epoch: 18, Loss: 14.512\n",
      "Epoch: 19, Loss: 12.001\n",
      "Epoch: 20, Loss: 9.809\n",
      "Epoch: 21, Loss: 8.268\n",
      "Epoch: 22, Loss: 11.764\n",
      "Epoch: 23, Loss: 9.718\n",
      "Epoch: 24, Loss: 9.471\n",
      "Epoch: 25, Loss: 8.988\n",
      "Epoch: 26, Loss: 9.592\n",
      "Epoch: 27, Loss: 8.738\n",
      "Epoch: 28, Loss: 8.382\n",
      "Epoch: 29, Loss: 9.516\n",
      "Epoch: 30, Loss: 9.224\n",
      "Epoch: 31, Loss: 9.124\n",
      "Epoch: 32, Loss: 8.508\n",
      "Epoch: 33, Loss: 6.893\n",
      "Epoch: 34, Loss: 8.669\n",
      "Epoch: 35, Loss: 6.753\n",
      "Epoch: 36, Loss: 8.717\n",
      "Epoch: 37, Loss: 7.540\n",
      "Epoch: 38, Loss: 8.093\n",
      "Epoch: 39, Loss: 7.137\n",
      "Epoch: 40, Loss: 6.420\n",
      "Epoch: 41, Loss: 8.423\n",
      "Epoch: 42, Loss: 7.742\n",
      "Epoch: 43, Loss: 6.771\n",
      "Epoch: 44, Loss: 7.807\n",
      "Epoch: 45, Loss: 6.926\n",
      "Epoch: 46, Loss: 6.727\n",
      "Epoch: 47, Loss: 7.725\n",
      "Epoch: 48, Loss: 7.781\n",
      "Epoch: 49, Loss: 6.376\n",
      "Epoch: 50, Loss: 6.974\n",
      "Epoch: 51, Loss: 7.293\n",
      "Epoch: 52, Loss: 8.712\n",
      "Epoch: 53, Loss: 6.860\n",
      "Epoch: 54, Loss: 7.376\n",
      "Epoch: 55, Loss: 6.125\n",
      "Epoch: 56, Loss: 6.463\n",
      "Epoch: 57, Loss: 7.245\n",
      "Epoch: 58, Loss: 5.382\n",
      "Epoch: 59, Loss: 6.268\n",
      "Epoch: 60, Loss: 6.509\n",
      "Epoch: 61, Loss: 6.887\n",
      "Epoch: 62, Loss: 6.389\n",
      "Epoch: 63, Loss: 5.501\n",
      "Epoch: 64, Loss: 7.672\n",
      "Epoch: 65, Loss: 6.993\n",
      "Epoch: 66, Loss: 6.397\n",
      "Epoch: 67, Loss: 5.682\n",
      "Epoch: 68, Loss: 5.911\n",
      "Epoch: 69, Loss: 7.465\n",
      "Epoch: 70, Loss: 7.034\n",
      "Epoch: 71, Loss: 6.714\n",
      "Epoch: 72, Loss: 5.167\n",
      "Epoch: 73, Loss: 5.818\n",
      "Epoch: 74, Loss: 6.331\n",
      "Epoch: 75, Loss: 5.901\n",
      "Epoch: 76, Loss: 6.458\n",
      "Epoch: 77, Loss: 8.061\n",
      "Epoch: 78, Loss: 7.915\n",
      "Epoch: 79, Loss: 6.225\n",
      "Epoch: 80, Loss: 5.977\n",
      "Epoch: 81, Loss: 5.005\n",
      "Epoch: 82, Loss: 5.865\n",
      "Epoch: 83, Loss: 5.397\n",
      "Epoch: 84, Loss: 7.342\n",
      "Epoch: 85, Loss: 5.023\n",
      "Epoch: 86, Loss: 6.314\n",
      "Epoch: 87, Loss: 5.769\n",
      "Epoch: 88, Loss: 6.765\n",
      "Epoch: 89, Loss: 6.051\n",
      "Epoch: 90, Loss: 4.871\n",
      "Epoch: 91, Loss: 6.774\n",
      "Epoch: 92, Loss: 5.741\n",
      "Epoch: 93, Loss: 5.976\n",
      "Epoch: 94, Loss: 5.787\n",
      "Epoch: 95, Loss: 7.139\n",
      "Epoch: 96, Loss: 5.566\n",
      "Epoch: 97, Loss: 6.429\n",
      "Epoch: 98, Loss: 5.211\n",
      "Epoch: 99, Loss: 5.930\n",
      "Epoch: 100, Loss: 6.302\n",
      "Epoch: 101, Loss: 4.930\n",
      "Epoch: 102, Loss: 6.845\n",
      "Epoch: 103, Loss: 6.249\n",
      "Epoch: 104, Loss: 5.059\n",
      "Epoch: 105, Loss: 5.874\n",
      "Epoch: 106, Loss: 5.301\n",
      "Epoch: 107, Loss: 7.172\n",
      "Epoch: 108, Loss: 5.118\n",
      "Epoch: 109, Loss: 5.925\n",
      "Epoch: 110, Loss: 6.067\n",
      "Epoch: 111, Loss: 5.587\n",
      "Epoch: 112, Loss: 5.436\n",
      "Epoch: 113, Loss: 7.024\n",
      "Epoch: 114, Loss: 5.416\n",
      "Epoch: 115, Loss: 4.633\n",
      "Epoch: 116, Loss: 5.642\n",
      "Epoch: 117, Loss: 5.849\n",
      "Epoch: 118, Loss: 6.190\n",
      "Epoch: 119, Loss: 4.974\n",
      "Epoch: 120, Loss: 6.326\n",
      "Epoch: 121, Loss: 5.720\n",
      "Epoch: 122, Loss: 5.731\n",
      "Epoch: 123, Loss: 5.761\n",
      "Epoch: 124, Loss: 5.141\n",
      "Epoch: 125, Loss: 6.137\n",
      "Epoch: 126, Loss: 5.107\n",
      "Epoch: 127, Loss: 5.204\n",
      "Epoch: 128, Loss: 5.398\n",
      "Epoch: 129, Loss: 6.216\n",
      "Epoch: 130, Loss: 5.912\n",
      "Epoch: 131, Loss: 5.068\n",
      "Epoch: 132, Loss: 5.333\n",
      "Epoch: 133, Loss: 6.481\n",
      "Epoch: 134, Loss: 4.491\n",
      "Epoch: 135, Loss: 5.001\n",
      "Epoch: 136, Loss: 4.833\n",
      "Epoch: 137, Loss: 6.079\n",
      "Epoch: 138, Loss: 5.060\n",
      "Epoch: 139, Loss: 5.323\n",
      "Epoch: 140, Loss: 5.839\n",
      "Epoch: 141, Loss: 6.368\n",
      "Epoch: 142, Loss: 6.328\n",
      "Epoch: 143, Loss: 6.256\n",
      "Epoch: 144, Loss: 5.710\n",
      "Epoch: 145, Loss: 5.102\n",
      "Epoch: 146, Loss: 5.850\n",
      "Epoch: 147, Loss: 6.019\n",
      "Epoch: 148, Loss: 4.988\n",
      "Epoch: 149, Loss: 5.920\n",
      "Epoch: 150, Loss: 5.639\n",
      "Epoch: 151, Loss: 5.195\n",
      "Epoch: 152, Loss: 4.265\n",
      "Epoch: 153, Loss: 5.292\n",
      "Epoch: 154, Loss: 4.940\n",
      "Epoch: 155, Loss: 4.933\n",
      "Epoch: 156, Loss: 5.127\n",
      "Epoch: 157, Loss: 4.951\n",
      "Epoch: 158, Loss: 5.628\n",
      "Epoch: 159, Loss: 4.568\n",
      "Epoch: 160, Loss: 6.160\n",
      "Epoch: 161, Loss: 5.605\n",
      "Epoch: 162, Loss: 5.310\n",
      "Epoch: 163, Loss: 5.765\n",
      "Epoch: 164, Loss: 4.661\n",
      "Epoch: 165, Loss: 6.026\n",
      "Epoch: 166, Loss: 3.935\n",
      "Epoch: 167, Loss: 5.362\n",
      "Epoch: 168, Loss: 5.534\n",
      "Epoch: 169, Loss: 5.003\n",
      "Epoch: 170, Loss: 5.556\n",
      "Epoch: 171, Loss: 4.245\n",
      "Epoch: 172, Loss: 4.273\n",
      "Epoch: 173, Loss: 5.746\n",
      "Epoch: 174, Loss: 4.699\n",
      "Epoch: 175, Loss: 5.942\n",
      "Epoch: 176, Loss: 5.105\n",
      "Epoch: 177, Loss: 5.141\n",
      "Epoch: 178, Loss: 5.419\n",
      "Epoch: 179, Loss: 4.795\n",
      "Epoch: 180, Loss: 4.592\n",
      "Epoch: 181, Loss: 5.999\n",
      "Epoch: 182, Loss: 4.745\n",
      "Epoch: 183, Loss: 6.309\n",
      "Epoch: 184, Loss: 5.285\n",
      "Epoch: 185, Loss: 4.515\n",
      "Epoch: 186, Loss: 4.893\n",
      "Epoch: 187, Loss: 4.458\n",
      "Epoch: 188, Loss: 4.729\n",
      "Epoch: 189, Loss: 5.884\n",
      "Epoch: 190, Loss: 3.894\n",
      "Epoch: 191, Loss: 6.116\n",
      "Epoch: 192, Loss: 4.853\n",
      "Epoch: 193, Loss: 5.880\n",
      "Epoch: 194, Loss: 5.434\n",
      "Epoch: 195, Loss: 4.432\n",
      "Epoch: 196, Loss: 4.421\n",
      "Epoch: 197, Loss: 5.846\n",
      "Epoch: 198, Loss: 4.999\n",
      "Epoch: 199, Loss: 4.945\n",
      "Epoch: 200, Loss: 5.063\n",
      "Epoch: 201, Loss: 4.743\n",
      "Epoch: 202, Loss: 5.008\n",
      "Epoch: 203, Loss: 5.204\n",
      "Epoch: 204, Loss: 4.527\n",
      "Epoch: 205, Loss: 4.218\n",
      "Epoch: 206, Loss: 5.705\n",
      "Epoch: 207, Loss: 4.794\n",
      "Epoch: 208, Loss: 4.700\n",
      "Epoch: 209, Loss: 5.039\n",
      "Epoch: 210, Loss: 4.353\n",
      "Epoch: 211, Loss: 4.618\n",
      "Epoch: 212, Loss: 4.756\n",
      "Epoch: 213, Loss: 4.875\n",
      "Epoch: 214, Loss: 5.601\n",
      "Epoch: 215, Loss: 4.705\n",
      "Epoch: 216, Loss: 6.396\n",
      "Epoch: 217, Loss: 4.685\n",
      "Epoch: 218, Loss: 4.871\n",
      "Epoch: 219, Loss: 5.055\n",
      "Epoch: 220, Loss: 5.785\n",
      "Epoch: 221, Loss: 5.335\n",
      "Epoch: 222, Loss: 5.588\n",
      "Epoch: 223, Loss: 5.631\n",
      "Epoch: 224, Loss: 4.688\n",
      "Epoch: 225, Loss: 4.815\n",
      "Epoch: 226, Loss: 5.009\n",
      "Epoch: 227, Loss: 3.779\n",
      "Epoch: 228, Loss: 5.650\n",
      "Epoch: 229, Loss: 4.792\n",
      "Epoch: 230, Loss: 4.587\n",
      "Epoch: 231, Loss: 4.885\n",
      "Epoch: 232, Loss: 4.515\n",
      "Epoch: 233, Loss: 5.120\n",
      "Epoch: 234, Loss: 5.823\n",
      "Epoch: 235, Loss: 4.974\n",
      "Epoch: 236, Loss: 3.809\n",
      "Epoch: 237, Loss: 4.972\n",
      "Epoch: 238, Loss: 5.599\n",
      "Epoch: 239, Loss: 5.960\n",
      "Epoch: 240, Loss: 4.825\n",
      "Epoch: 241, Loss: 4.790\n",
      "Epoch: 242, Loss: 4.426\n",
      "Epoch: 243, Loss: 4.763\n",
      "Epoch: 244, Loss: 5.435\n",
      "Epoch: 245, Loss: 4.248\n",
      "Epoch: 246, Loss: 4.488\n",
      "Epoch: 247, Loss: 5.262\n",
      "Epoch: 248, Loss: 5.078\n",
      "Epoch: 249, Loss: 4.257\n",
      "Epoch: 250, Loss: 4.856\n",
      "Epoch: 251, Loss: 4.392\n",
      "Epoch: 252, Loss: 4.587\n",
      "Epoch: 253, Loss: 5.222\n",
      "Epoch: 254, Loss: 4.746\n",
      "Epoch: 255, Loss: 5.092\n",
      "Epoch: 256, Loss: 4.188\n",
      "Epoch: 257, Loss: 5.190\n",
      "Epoch: 258, Loss: 4.549\n",
      "Epoch: 259, Loss: 5.554\n",
      "Epoch: 260, Loss: 5.726\n",
      "Epoch: 261, Loss: 4.451\n",
      "Epoch: 262, Loss: 5.186\n",
      "Epoch: 263, Loss: 4.938\n",
      "Epoch: 264, Loss: 4.767\n",
      "Epoch: 265, Loss: 4.232\n",
      "Epoch: 266, Loss: 5.099\n",
      "Epoch: 267, Loss: 5.046\n",
      "Epoch: 268, Loss: 4.754\n",
      "Epoch: 269, Loss: 4.520\n",
      "Epoch: 270, Loss: 4.557\n",
      "Epoch: 271, Loss: 5.062\n",
      "Epoch: 272, Loss: 3.997\n",
      "Epoch: 273, Loss: 4.508\n",
      "Epoch: 274, Loss: 5.179\n",
      "Epoch: 275, Loss: 4.380\n",
      "Epoch: 276, Loss: 4.620\n",
      "Epoch: 277, Loss: 4.828\n",
      "Epoch: 278, Loss: 4.746\n",
      "Epoch: 279, Loss: 4.378\n",
      "Epoch: 280, Loss: 5.217\n",
      "Epoch: 281, Loss: 4.925\n",
      "Epoch: 282, Loss: 4.792\n",
      "Epoch: 283, Loss: 4.819\n",
      "Epoch: 284, Loss: 5.015\n",
      "Epoch: 285, Loss: 4.647\n",
      "Epoch: 286, Loss: 4.160\n",
      "Epoch: 287, Loss: 4.452\n",
      "Epoch: 288, Loss: 5.367\n",
      "Epoch: 289, Loss: 3.914\n",
      "Epoch: 290, Loss: 5.264\n",
      "Epoch: 291, Loss: 4.702\n",
      "Epoch: 292, Loss: 4.513\n",
      "Epoch: 293, Loss: 3.962\n",
      "Epoch: 294, Loss: 5.181\n",
      "Epoch: 295, Loss: 4.862\n",
      "Epoch: 296, Loss: 4.635\n",
      "Epoch: 297, Loss: 4.384\n",
      "Epoch: 298, Loss: 4.169\n",
      "Epoch: 299, Loss: 4.889\n",
      "Epoch: 300, Loss: 4.335\n",
      "Epoch: 301, Loss: 4.685\n",
      "Epoch: 302, Loss: 4.703\n",
      "Epoch: 303, Loss: 5.533\n",
      "Epoch: 304, Loss: 4.927\n",
      "Epoch: 305, Loss: 4.789\n",
      "Epoch: 306, Loss: 5.440\n",
      "Epoch: 307, Loss: 4.557\n",
      "Epoch: 308, Loss: 4.253\n",
      "Epoch: 309, Loss: 5.223\n",
      "Epoch: 310, Loss: 4.158\n",
      "Epoch: 311, Loss: 4.253\n",
      "Epoch: 312, Loss: 4.469\n",
      "Epoch: 313, Loss: 5.595\n",
      "Epoch: 314, Loss: 3.953\n",
      "Epoch: 315, Loss: 4.292\n",
      "Epoch: 316, Loss: 4.627\n",
      "Epoch: 317, Loss: 5.784\n",
      "Epoch: 318, Loss: 4.968\n",
      "Epoch: 319, Loss: 4.674\n",
      "Epoch: 320, Loss: 4.193\n",
      "Epoch: 321, Loss: 4.347\n",
      "Epoch: 322, Loss: 4.762\n",
      "Epoch: 323, Loss: 4.781\n",
      "Epoch: 324, Loss: 4.639\n",
      "Epoch: 325, Loss: 5.738\n",
      "Epoch: 326, Loss: 4.613\n",
      "Epoch: 327, Loss: 4.945\n",
      "Epoch: 328, Loss: 4.384\n",
      "Epoch: 329, Loss: 4.499\n",
      "Epoch: 330, Loss: 5.054\n",
      "Epoch: 331, Loss: 4.883\n",
      "Epoch: 332, Loss: 4.170\n",
      "Epoch: 333, Loss: 5.074\n",
      "Epoch: 334, Loss: 4.900\n",
      "Epoch: 335, Loss: 5.116\n",
      "Epoch: 336, Loss: 4.629\n",
      "Epoch: 337, Loss: 4.475\n",
      "Epoch: 338, Loss: 4.219\n",
      "Epoch: 339, Loss: 4.491\n",
      "Epoch: 340, Loss: 4.479\n",
      "Epoch: 341, Loss: 4.652\n",
      "Epoch: 342, Loss: 4.987\n",
      "Epoch: 343, Loss: 4.232\n",
      "Epoch: 344, Loss: 3.852\n",
      "Epoch: 345, Loss: 5.424\n",
      "Epoch: 346, Loss: 4.903\n",
      "Epoch: 347, Loss: 3.970\n",
      "Epoch: 348, Loss: 4.836\n",
      "Epoch: 349, Loss: 3.896\n",
      "Epoch: 350, Loss: 4.127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 351, Loss: 3.910\n",
      "Epoch: 352, Loss: 4.412\n",
      "Epoch: 353, Loss: 5.476\n",
      "Epoch: 354, Loss: 4.244\n",
      "Epoch: 355, Loss: 4.594\n",
      "Epoch: 356, Loss: 4.009\n",
      "Epoch: 357, Loss: 4.930\n",
      "Epoch: 358, Loss: 4.735\n",
      "Epoch: 359, Loss: 4.445\n",
      "Epoch: 360, Loss: 5.347\n",
      "Epoch: 361, Loss: 3.660\n",
      "Epoch: 362, Loss: 4.690\n",
      "Epoch: 363, Loss: 4.667\n",
      "Epoch: 364, Loss: 4.738\n",
      "Epoch: 365, Loss: 5.244\n",
      "Epoch: 366, Loss: 4.898\n",
      "Epoch: 367, Loss: 4.959\n",
      "Epoch: 368, Loss: 4.967\n",
      "Epoch: 369, Loss: 4.923\n",
      "Epoch: 370, Loss: 5.399\n",
      "Epoch: 371, Loss: 4.284\n",
      "Epoch: 372, Loss: 5.348\n",
      "Epoch: 373, Loss: 5.420\n",
      "Epoch: 374, Loss: 4.455\n",
      "Epoch: 375, Loss: 4.487\n",
      "Epoch: 376, Loss: 4.556\n",
      "Epoch: 377, Loss: 4.046\n",
      "Epoch: 378, Loss: 4.871\n",
      "Epoch: 379, Loss: 4.717\n",
      "Epoch: 380, Loss: 4.412\n",
      "Epoch: 381, Loss: 4.865\n",
      "Epoch: 382, Loss: 4.204\n",
      "Epoch: 383, Loss: 4.753\n",
      "Epoch: 384, Loss: 4.674\n",
      "Epoch: 385, Loss: 3.974\n",
      "Epoch: 386, Loss: 4.238\n",
      "Epoch: 387, Loss: 4.329\n",
      "Epoch: 388, Loss: 5.247\n",
      "Epoch: 389, Loss: 4.856\n",
      "Epoch: 390, Loss: 4.900\n",
      "Epoch: 391, Loss: 4.534\n",
      "Epoch: 392, Loss: 4.388\n",
      "Epoch: 393, Loss: 4.653\n",
      "Epoch: 394, Loss: 4.303\n",
      "Epoch: 395, Loss: 4.691\n",
      "Epoch: 396, Loss: 4.902\n",
      "Epoch: 397, Loss: 4.373\n",
      "Epoch: 398, Loss: 4.618\n",
      "Epoch: 399, Loss: 4.396\n",
      "Epoch: 400, Loss: 4.392\n",
      "Epoch: 401, Loss: 4.694\n",
      "Epoch: 402, Loss: 5.320\n",
      "Epoch: 403, Loss: 5.097\n",
      "Epoch: 404, Loss: 5.734\n",
      "Epoch: 405, Loss: 4.229\n",
      "Epoch: 406, Loss: 4.167\n",
      "Epoch: 407, Loss: 4.499\n",
      "Epoch: 408, Loss: 4.812\n",
      "Epoch: 409, Loss: 3.717\n",
      "Epoch: 410, Loss: 4.527\n",
      "Epoch: 411, Loss: 4.039\n",
      "Epoch: 412, Loss: 4.469\n",
      "Epoch: 413, Loss: 4.120\n",
      "Epoch: 414, Loss: 4.189\n",
      "Epoch: 415, Loss: 5.441\n",
      "Epoch: 416, Loss: 4.993\n",
      "Epoch: 417, Loss: 4.059\n",
      "Epoch: 418, Loss: 4.507\n",
      "Epoch: 419, Loss: 3.965\n",
      "Epoch: 420, Loss: 5.060\n",
      "Epoch: 421, Loss: 4.024\n",
      "Epoch: 422, Loss: 4.071\n",
      "Epoch: 423, Loss: 5.241\n",
      "Epoch: 424, Loss: 4.560\n",
      "Epoch: 425, Loss: 5.581\n",
      "Epoch: 426, Loss: 4.463\n",
      "Epoch: 427, Loss: 4.059\n",
      "Epoch: 428, Loss: 4.393\n",
      "Epoch: 429, Loss: 3.282\n",
      "Epoch: 430, Loss: 4.471\n",
      "Epoch: 431, Loss: 3.958\n",
      "Epoch: 432, Loss: 3.787\n",
      "Epoch: 433, Loss: 4.118\n",
      "Epoch: 434, Loss: 3.976\n",
      "Epoch: 435, Loss: 4.027\n",
      "Epoch: 436, Loss: 5.117\n",
      "Epoch: 437, Loss: 4.277\n",
      "Epoch: 438, Loss: 4.006\n",
      "Epoch: 439, Loss: 4.412\n",
      "Epoch: 440, Loss: 4.953\n",
      "Epoch: 441, Loss: 4.231\n",
      "Epoch: 442, Loss: 5.431\n",
      "Epoch: 443, Loss: 4.918\n",
      "Epoch: 444, Loss: 4.690\n",
      "Epoch: 445, Loss: 4.279\n",
      "Epoch: 446, Loss: 4.317\n",
      "Epoch: 447, Loss: 4.651\n",
      "Epoch: 448, Loss: 4.643\n",
      "Epoch: 449, Loss: 5.526\n",
      "Epoch: 450, Loss: 4.902\n",
      "Epoch: 451, Loss: 4.732\n",
      "Epoch: 452, Loss: 4.919\n",
      "Epoch: 453, Loss: 4.097\n",
      "Epoch: 454, Loss: 4.762\n",
      "Epoch: 455, Loss: 4.813\n",
      "Epoch: 456, Loss: 4.082\n",
      "Epoch: 457, Loss: 4.481\n",
      "Epoch: 458, Loss: 5.260\n",
      "Epoch: 459, Loss: 5.105\n",
      "Epoch: 460, Loss: 3.990\n",
      "Epoch: 461, Loss: 4.770\n",
      "Epoch: 462, Loss: 3.738\n",
      "Epoch: 463, Loss: 4.746\n",
      "Epoch: 464, Loss: 4.563\n",
      "Epoch: 465, Loss: 4.767\n",
      "Epoch: 466, Loss: 3.811\n",
      "Epoch: 467, Loss: 4.075\n",
      "Epoch: 468, Loss: 4.090\n",
      "Epoch: 469, Loss: 4.614\n",
      "Epoch: 470, Loss: 4.063\n",
      "Epoch: 471, Loss: 4.221\n",
      "Epoch: 472, Loss: 4.058\n",
      "Epoch: 473, Loss: 4.607\n",
      "Epoch: 474, Loss: 4.916\n",
      "Epoch: 475, Loss: 4.306\n",
      "Epoch: 476, Loss: 4.325\n",
      "Epoch: 477, Loss: 4.445\n",
      "Epoch: 478, Loss: 4.797\n",
      "Epoch: 479, Loss: 4.085\n",
      "Epoch: 480, Loss: 3.549\n",
      "Epoch: 481, Loss: 4.848\n",
      "Epoch: 482, Loss: 4.452\n",
      "Epoch: 483, Loss: 4.245\n",
      "Epoch: 484, Loss: 4.002\n",
      "Epoch: 485, Loss: 4.276\n",
      "Epoch: 486, Loss: 3.982\n",
      "Epoch: 487, Loss: 4.578\n",
      "Epoch: 488, Loss: 3.757\n",
      "Epoch: 489, Loss: 4.611\n",
      "Epoch: 490, Loss: 4.114\n",
      "Epoch: 491, Loss: 4.244\n",
      "Epoch: 492, Loss: 4.301\n",
      "Epoch: 493, Loss: 4.825\n",
      "Epoch: 494, Loss: 4.173\n",
      "Epoch: 495, Loss: 4.829\n",
      "Epoch: 496, Loss: 4.844\n",
      "Epoch: 497, Loss: 5.186\n",
      "Epoch: 498, Loss: 3.880\n",
      "Epoch: 499, Loss: 4.423\n",
      "Epoch: 500, Loss: 4.838\n",
      "Epoch: 501, Loss: 4.465\n",
      "Epoch: 502, Loss: 3.478\n",
      "Epoch: 503, Loss: 4.197\n",
      "Epoch: 504, Loss: 4.910\n",
      "Epoch: 505, Loss: 3.928\n",
      "Epoch: 506, Loss: 4.990\n",
      "Epoch: 507, Loss: 4.343\n",
      "Epoch: 508, Loss: 4.398\n",
      "Epoch: 509, Loss: 4.068\n",
      "Epoch: 510, Loss: 4.464\n",
      "Epoch: 511, Loss: 3.898\n",
      "Epoch: 512, Loss: 3.676\n",
      "Epoch: 513, Loss: 3.739\n",
      "Epoch: 514, Loss: 4.043\n",
      "Epoch: 515, Loss: 4.424\n",
      "Epoch: 516, Loss: 4.338\n",
      "Epoch: 517, Loss: 4.170\n",
      "Epoch: 518, Loss: 3.281\n",
      "Epoch: 519, Loss: 4.269\n",
      "Epoch: 520, Loss: 3.730\n",
      "Epoch: 521, Loss: 4.798\n",
      "Epoch: 522, Loss: 4.955\n",
      "Epoch: 523, Loss: 3.618\n",
      "Epoch: 524, Loss: 4.448\n",
      "Epoch: 525, Loss: 4.652\n",
      "Epoch: 526, Loss: 3.992\n",
      "Epoch: 527, Loss: 4.507\n",
      "Epoch: 528, Loss: 4.041\n",
      "Epoch: 529, Loss: 4.037\n",
      "Epoch: 530, Loss: 3.924\n",
      "Epoch: 531, Loss: 4.226\n",
      "Epoch: 532, Loss: 4.481\n",
      "Epoch: 533, Loss: 3.911\n",
      "Epoch: 534, Loss: 4.209\n",
      "Epoch: 535, Loss: 4.341\n",
      "Epoch: 536, Loss: 4.502\n",
      "Epoch: 537, Loss: 4.140\n",
      "Epoch: 538, Loss: 4.203\n",
      "Epoch: 539, Loss: 3.922\n",
      "Epoch: 540, Loss: 4.420\n",
      "Epoch: 541, Loss: 4.436\n",
      "Epoch: 542, Loss: 4.795\n",
      "Epoch: 543, Loss: 4.551\n",
      "Epoch: 544, Loss: 3.946\n",
      "Epoch: 545, Loss: 4.571\n",
      "Epoch: 546, Loss: 4.400\n",
      "Epoch: 547, Loss: 4.107\n",
      "Epoch: 548, Loss: 4.353\n",
      "Epoch: 549, Loss: 4.134\n",
      "Epoch: 550, Loss: 4.673\n",
      "Epoch: 551, Loss: 4.025\n",
      "Epoch: 552, Loss: 4.376\n",
      "Epoch: 553, Loss: 4.176\n",
      "Epoch: 554, Loss: 4.410\n",
      "Epoch: 555, Loss: 4.029\n",
      "Epoch: 556, Loss: 4.392\n",
      "Epoch: 557, Loss: 4.236\n",
      "Epoch: 558, Loss: 4.895\n",
      "Epoch: 559, Loss: 3.867\n",
      "Epoch: 560, Loss: 4.892\n",
      "Epoch: 561, Loss: 4.378\n",
      "Epoch: 562, Loss: 4.948\n",
      "Epoch: 563, Loss: 4.544\n",
      "Epoch: 564, Loss: 3.949\n",
      "Epoch: 565, Loss: 4.525\n",
      "Epoch: 566, Loss: 4.257\n",
      "Epoch: 567, Loss: 4.037\n",
      "Epoch: 568, Loss: 3.981\n",
      "Epoch: 569, Loss: 4.010\n",
      "Epoch: 570, Loss: 4.409\n",
      "Epoch: 571, Loss: 4.739\n",
      "Epoch: 572, Loss: 4.249\n",
      "Epoch: 573, Loss: 4.254\n",
      "Epoch: 574, Loss: 4.143\n",
      "Epoch: 575, Loss: 4.633\n",
      "Epoch: 576, Loss: 4.494\n",
      "Epoch: 577, Loss: 4.351\n",
      "Epoch: 578, Loss: 4.481\n",
      "Epoch: 579, Loss: 3.988\n",
      "Epoch: 580, Loss: 4.326\n",
      "Epoch: 581, Loss: 5.202\n",
      "Epoch: 582, Loss: 3.788\n",
      "Epoch: 583, Loss: 4.596\n",
      "Epoch: 584, Loss: 4.700\n",
      "Epoch: 585, Loss: 4.642\n",
      "Epoch: 586, Loss: 4.431\n",
      "Epoch: 587, Loss: 4.839\n",
      "Epoch: 588, Loss: 4.513\n",
      "Epoch: 589, Loss: 4.659\n",
      "Epoch: 590, Loss: 4.510\n",
      "Epoch: 591, Loss: 4.406\n",
      "Epoch: 592, Loss: 4.302\n",
      "Epoch: 593, Loss: 3.899\n",
      "Epoch: 594, Loss: 4.810\n",
      "Epoch: 595, Loss: 3.563\n",
      "Epoch: 596, Loss: 4.299\n",
      "Epoch: 597, Loss: 5.018\n",
      "Epoch: 598, Loss: 4.755\n",
      "Epoch: 599, Loss: 3.563\n",
      "Epoch: 600, Loss: 3.904\n",
      "Epoch: 601, Loss: 4.182\n",
      "Epoch: 602, Loss: 4.225\n",
      "Epoch: 603, Loss: 4.033\n",
      "Epoch: 604, Loss: 4.721\n",
      "Epoch: 605, Loss: 4.364\n",
      "Epoch: 606, Loss: 4.299\n",
      "Epoch: 607, Loss: 5.190\n",
      "Epoch: 608, Loss: 3.968\n",
      "Epoch: 609, Loss: 4.654\n",
      "Epoch: 610, Loss: 4.609\n",
      "Epoch: 611, Loss: 4.366\n",
      "Epoch: 612, Loss: 4.404\n",
      "Epoch: 613, Loss: 4.193\n",
      "Epoch: 614, Loss: 4.538\n",
      "Epoch: 615, Loss: 3.361\n",
      "Epoch: 616, Loss: 3.988\n",
      "Epoch: 617, Loss: 4.165\n",
      "Epoch: 618, Loss: 4.267\n",
      "Epoch: 619, Loss: 4.162\n",
      "Epoch: 620, Loss: 3.744\n",
      "Epoch: 621, Loss: 4.487\n",
      "Epoch: 622, Loss: 4.301\n",
      "Epoch: 623, Loss: 4.594\n",
      "Epoch: 624, Loss: 4.109\n",
      "Epoch: 625, Loss: 3.925\n",
      "Epoch: 626, Loss: 4.167\n",
      "Epoch: 627, Loss: 4.655\n",
      "Epoch: 628, Loss: 3.973\n",
      "Epoch: 629, Loss: 4.116\n",
      "Epoch: 630, Loss: 4.320\n",
      "Epoch: 631, Loss: 3.782\n",
      "Epoch: 632, Loss: 4.423\n",
      "Epoch: 633, Loss: 4.364\n",
      "Epoch: 634, Loss: 4.127\n",
      "Epoch: 635, Loss: 4.379\n",
      "Epoch: 636, Loss: 4.761\n",
      "Epoch: 637, Loss: 4.230\n",
      "Epoch: 638, Loss: 4.442\n",
      "Epoch: 639, Loss: 4.484\n",
      "Epoch: 640, Loss: 3.991\n",
      "Epoch: 641, Loss: 4.267\n",
      "Epoch: 642, Loss: 3.630\n",
      "Epoch: 643, Loss: 4.871\n",
      "Epoch: 644, Loss: 4.117\n",
      "Epoch: 645, Loss: 3.931\n",
      "Epoch: 646, Loss: 4.587\n",
      "Epoch: 647, Loss: 4.649\n",
      "Epoch: 648, Loss: 3.632\n",
      "Epoch: 649, Loss: 4.745\n",
      "Epoch: 650, Loss: 5.093\n",
      "Epoch: 651, Loss: 4.787\n",
      "Epoch: 652, Loss: 5.072\n",
      "Epoch: 653, Loss: 4.389\n",
      "Epoch: 654, Loss: 4.152\n",
      "Epoch: 655, Loss: 4.292\n",
      "Epoch: 656, Loss: 4.943\n",
      "Epoch: 657, Loss: 4.112\n",
      "Epoch: 658, Loss: 4.267\n",
      "Epoch: 659, Loss: 3.645\n",
      "Epoch: 660, Loss: 3.599\n",
      "Epoch: 661, Loss: 4.348\n",
      "Epoch: 662, Loss: 4.602\n",
      "Epoch: 663, Loss: 4.370\n",
      "Epoch: 664, Loss: 4.591\n",
      "Epoch: 665, Loss: 3.604\n",
      "Epoch: 666, Loss: 4.416\n",
      "Epoch: 667, Loss: 3.470\n",
      "Epoch: 668, Loss: 4.159\n",
      "Epoch: 669, Loss: 4.328\n",
      "Epoch: 670, Loss: 3.433\n",
      "Epoch: 671, Loss: 3.766\n",
      "Epoch: 672, Loss: 3.983\n",
      "Epoch: 673, Loss: 3.996\n",
      "Epoch: 674, Loss: 4.262\n",
      "Epoch: 675, Loss: 3.753\n",
      "Epoch: 676, Loss: 4.256\n",
      "Epoch: 677, Loss: 4.919\n",
      "Epoch: 678, Loss: 4.428\n",
      "Epoch: 679, Loss: 5.267\n",
      "Epoch: 680, Loss: 4.457\n",
      "Epoch: 681, Loss: 4.934\n",
      "Epoch: 682, Loss: 3.975\n",
      "Epoch: 683, Loss: 4.210\n",
      "Epoch: 684, Loss: 4.071\n",
      "Epoch: 685, Loss: 4.009\n",
      "Epoch: 686, Loss: 4.257\n",
      "Epoch: 687, Loss: 3.777\n",
      "Epoch: 688, Loss: 4.321\n",
      "Epoch: 689, Loss: 3.868\n",
      "Epoch: 690, Loss: 4.477\n",
      "Epoch: 691, Loss: 4.045\n",
      "Epoch: 692, Loss: 3.779\n",
      "Epoch: 693, Loss: 3.502\n",
      "Epoch: 694, Loss: 4.711\n",
      "Epoch: 695, Loss: 3.549\n",
      "Epoch: 696, Loss: 4.267\n",
      "Epoch: 697, Loss: 4.010\n",
      "Epoch: 698, Loss: 4.058\n",
      "Epoch: 699, Loss: 3.690\n",
      "Epoch: 700, Loss: 4.724\n",
      "Epoch: 701, Loss: 4.538\n",
      "Epoch: 702, Loss: 4.272\n",
      "Epoch: 703, Loss: 4.228\n",
      "Epoch: 704, Loss: 3.854\n",
      "Epoch: 705, Loss: 4.787\n",
      "Epoch: 706, Loss: 4.443\n",
      "Epoch: 707, Loss: 3.714\n",
      "Epoch: 708, Loss: 4.261\n",
      "Epoch: 709, Loss: 4.094\n",
      "Epoch: 710, Loss: 3.592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 711, Loss: 3.987\n",
      "Epoch: 712, Loss: 3.783\n",
      "Epoch: 713, Loss: 4.506\n",
      "Epoch: 714, Loss: 3.638\n",
      "Epoch: 715, Loss: 4.334\n",
      "Epoch: 716, Loss: 4.261\n",
      "Epoch: 717, Loss: 4.142\n",
      "Epoch: 718, Loss: 4.321\n",
      "Epoch: 719, Loss: 4.314\n",
      "Epoch: 720, Loss: 4.105\n",
      "Epoch: 721, Loss: 4.195\n",
      "Epoch: 722, Loss: 4.302\n",
      "Epoch: 723, Loss: 3.723\n",
      "Epoch: 724, Loss: 4.265\n",
      "Epoch: 725, Loss: 4.203\n",
      "Epoch: 726, Loss: 3.686\n",
      "Epoch: 727, Loss: 3.616\n",
      "Epoch: 728, Loss: 3.596\n",
      "Epoch: 729, Loss: 4.185\n",
      "Epoch: 730, Loss: 3.889\n",
      "Epoch: 731, Loss: 4.260\n",
      "Epoch: 732, Loss: 3.545\n",
      "Epoch: 733, Loss: 4.456\n",
      "Epoch: 734, Loss: 3.395\n",
      "Epoch: 735, Loss: 4.525\n",
      "Epoch: 736, Loss: 4.037\n",
      "Epoch: 737, Loss: 3.985\n",
      "Epoch: 738, Loss: 4.075\n",
      "Epoch: 739, Loss: 3.656\n",
      "Epoch: 740, Loss: 4.770\n",
      "Epoch: 741, Loss: 4.617\n",
      "Epoch: 742, Loss: 4.360\n",
      "Epoch: 743, Loss: 4.337\n",
      "Epoch: 744, Loss: 3.840\n",
      "Epoch: 745, Loss: 4.180\n",
      "Epoch: 746, Loss: 3.998\n",
      "Epoch: 747, Loss: 4.425\n",
      "Epoch: 748, Loss: 4.196\n",
      "Epoch: 749, Loss: 4.272\n",
      "Epoch: 750, Loss: 4.433\n",
      "Epoch: 751, Loss: 4.177\n",
      "Epoch: 752, Loss: 4.179\n",
      "Epoch: 753, Loss: 3.561\n",
      "Epoch: 754, Loss: 4.652\n",
      "Epoch: 755, Loss: 3.460\n",
      "Epoch: 756, Loss: 3.863\n",
      "Epoch: 757, Loss: 4.255\n",
      "Epoch: 758, Loss: 3.765\n",
      "Epoch: 759, Loss: 3.865\n",
      "Epoch: 760, Loss: 4.466\n",
      "Epoch: 761, Loss: 4.420\n",
      "Epoch: 762, Loss: 4.380\n",
      "Epoch: 763, Loss: 4.978\n",
      "Epoch: 764, Loss: 4.504\n",
      "Epoch: 765, Loss: 3.780\n",
      "Epoch: 766, Loss: 4.192\n",
      "Epoch: 767, Loss: 3.854\n",
      "Epoch: 768, Loss: 3.797\n",
      "Epoch: 769, Loss: 3.596\n",
      "Epoch: 770, Loss: 3.892\n",
      "Epoch: 771, Loss: 3.923\n",
      "Epoch: 772, Loss: 4.621\n",
      "Epoch: 773, Loss: 4.266\n",
      "Epoch: 774, Loss: 4.215\n",
      "Epoch: 775, Loss: 4.326\n",
      "Epoch: 776, Loss: 4.485\n",
      "Epoch: 777, Loss: 3.433\n",
      "Epoch: 778, Loss: 4.438\n",
      "Epoch: 779, Loss: 3.618\n",
      "Epoch: 780, Loss: 4.809\n",
      "Epoch: 781, Loss: 4.329\n",
      "Epoch: 782, Loss: 4.334\n",
      "Epoch: 783, Loss: 3.770\n",
      "Epoch: 784, Loss: 4.420\n",
      "Epoch: 785, Loss: 4.317\n",
      "Epoch: 786, Loss: 4.383\n",
      "Epoch: 787, Loss: 3.789\n",
      "Epoch: 788, Loss: 4.696\n",
      "Epoch: 789, Loss: 4.679\n",
      "Epoch: 790, Loss: 3.906\n",
      "Epoch: 791, Loss: 4.806\n",
      "Epoch: 792, Loss: 4.478\n",
      "Epoch: 793, Loss: 3.727\n",
      "Epoch: 794, Loss: 4.304\n",
      "Epoch: 795, Loss: 3.972\n",
      "Epoch: 796, Loss: 4.235\n",
      "Epoch: 797, Loss: 3.651\n",
      "Epoch: 798, Loss: 4.576\n",
      "Epoch: 799, Loss: 3.700\n",
      "Epoch: 800, Loss: 4.429\n",
      "Epoch: 801, Loss: 4.157\n",
      "Epoch: 802, Loss: 3.753\n",
      "Epoch: 803, Loss: 5.001\n",
      "Epoch: 804, Loss: 4.469\n",
      "Epoch: 805, Loss: 3.815\n",
      "Epoch: 806, Loss: 4.222\n",
      "Epoch: 807, Loss: 3.653\n",
      "Epoch: 808, Loss: 3.929\n",
      "Epoch: 809, Loss: 4.030\n",
      "Epoch: 810, Loss: 3.786\n",
      "Epoch: 811, Loss: 3.898\n",
      "Epoch: 812, Loss: 4.505\n",
      "Epoch: 813, Loss: 3.545\n",
      "Epoch: 814, Loss: 4.329\n",
      "Epoch: 815, Loss: 3.523\n",
      "Epoch: 816, Loss: 4.156\n",
      "Epoch: 817, Loss: 3.421\n",
      "Epoch: 818, Loss: 4.103\n",
      "Epoch: 819, Loss: 4.071\n",
      "Epoch: 820, Loss: 3.677\n",
      "Epoch: 821, Loss: 3.943\n",
      "Epoch: 822, Loss: 3.538\n",
      "Epoch: 823, Loss: 3.910\n",
      "Epoch: 824, Loss: 4.480\n",
      "Epoch: 825, Loss: 3.381\n",
      "Epoch: 826, Loss: 4.296\n",
      "Epoch: 827, Loss: 3.655\n",
      "Epoch: 828, Loss: 3.658\n",
      "Epoch: 829, Loss: 3.492\n",
      "Epoch: 830, Loss: 4.009\n",
      "Epoch: 831, Loss: 3.708\n",
      "Epoch: 832, Loss: 3.980\n",
      "Epoch: 833, Loss: 4.038\n",
      "Epoch: 834, Loss: 3.920\n",
      "Epoch: 835, Loss: 4.403\n",
      "Epoch: 836, Loss: 3.497\n",
      "Epoch: 837, Loss: 3.833\n",
      "Epoch: 838, Loss: 4.808\n",
      "Epoch: 839, Loss: 4.564\n",
      "Epoch: 840, Loss: 3.262\n",
      "Epoch: 841, Loss: 3.248\n",
      "Epoch: 842, Loss: 3.757\n",
      "Epoch: 843, Loss: 4.222\n",
      "Epoch: 844, Loss: 3.854\n",
      "Epoch: 845, Loss: 4.036\n",
      "Epoch: 846, Loss: 3.817\n",
      "Epoch: 847, Loss: 3.776\n",
      "Epoch: 848, Loss: 3.757\n",
      "Epoch: 849, Loss: 3.397\n",
      "Epoch: 850, Loss: 4.409\n",
      "Epoch: 851, Loss: 4.116\n",
      "Epoch: 852, Loss: 4.661\n",
      "Epoch: 853, Loss: 3.777\n",
      "Epoch: 854, Loss: 3.768\n",
      "Epoch: 855, Loss: 3.614\n",
      "Epoch: 856, Loss: 4.197\n",
      "Epoch: 857, Loss: 3.941\n",
      "Epoch: 858, Loss: 4.326\n",
      "Epoch: 859, Loss: 3.489\n",
      "Epoch: 860, Loss: 3.971\n",
      "Epoch: 861, Loss: 4.075\n",
      "Epoch: 862, Loss: 4.295\n",
      "Epoch: 863, Loss: 3.768\n",
      "Epoch: 864, Loss: 3.800\n",
      "Epoch: 865, Loss: 3.938\n",
      "Epoch: 866, Loss: 4.069\n",
      "Epoch: 867, Loss: 3.735\n",
      "Epoch: 868, Loss: 4.237\n",
      "Epoch: 869, Loss: 3.716\n",
      "Epoch: 870, Loss: 3.424\n",
      "Epoch: 871, Loss: 3.908\n",
      "Epoch: 872, Loss: 3.634\n",
      "Epoch: 873, Loss: 4.036\n",
      "Epoch: 874, Loss: 3.608\n",
      "Epoch: 875, Loss: 4.294\n",
      "Epoch: 876, Loss: 3.059\n",
      "Epoch: 877, Loss: 3.718\n",
      "Epoch: 878, Loss: 3.942\n",
      "Epoch: 879, Loss: 4.418\n",
      "Epoch: 880, Loss: 4.258\n",
      "Epoch: 881, Loss: 3.197\n",
      "Epoch: 882, Loss: 4.381\n",
      "Epoch: 883, Loss: 3.663\n",
      "Epoch: 884, Loss: 4.372\n",
      "Epoch: 885, Loss: 2.980\n",
      "Epoch: 886, Loss: 3.986\n",
      "Epoch: 887, Loss: 3.293\n",
      "Epoch: 888, Loss: 4.091\n",
      "Epoch: 889, Loss: 3.938\n",
      "Epoch: 890, Loss: 4.498\n",
      "Epoch: 891, Loss: 3.712\n",
      "Epoch: 892, Loss: 3.840\n",
      "Epoch: 893, Loss: 3.884\n",
      "Epoch: 894, Loss: 3.970\n",
      "Epoch: 895, Loss: 3.790\n",
      "Epoch: 896, Loss: 3.730\n",
      "Epoch: 897, Loss: 4.035\n",
      "Epoch: 898, Loss: 4.136\n",
      "Epoch: 899, Loss: 4.063\n",
      "Epoch: 900, Loss: 3.892\n",
      "Epoch: 901, Loss: 3.237\n",
      "Epoch: 902, Loss: 3.573\n",
      "Epoch: 903, Loss: 3.489\n",
      "Epoch: 904, Loss: 4.055\n",
      "Epoch: 905, Loss: 3.509\n",
      "Epoch: 906, Loss: 3.652\n",
      "Epoch: 907, Loss: 3.800\n",
      "Epoch: 908, Loss: 3.289\n",
      "Epoch: 909, Loss: 4.068\n",
      "Epoch: 910, Loss: 3.444\n",
      "Epoch: 911, Loss: 2.970\n",
      "Epoch: 912, Loss: 4.178\n",
      "Epoch: 913, Loss: 3.803\n",
      "Epoch: 914, Loss: 4.990\n",
      "Epoch: 915, Loss: 3.605\n",
      "Epoch: 916, Loss: 3.659\n",
      "Epoch: 917, Loss: 3.144\n",
      "Epoch: 918, Loss: 3.763\n",
      "Epoch: 919, Loss: 3.660\n",
      "Epoch: 920, Loss: 3.792\n",
      "Epoch: 921, Loss: 3.710\n",
      "Epoch: 922, Loss: 3.870\n",
      "Epoch: 923, Loss: 3.650\n",
      "Epoch: 924, Loss: 3.900\n",
      "Epoch: 925, Loss: 3.700\n",
      "Epoch: 926, Loss: 3.924\n",
      "Epoch: 927, Loss: 4.348\n",
      "Epoch: 928, Loss: 3.367\n",
      "Epoch: 929, Loss: 4.017\n",
      "Epoch: 930, Loss: 4.619\n",
      "Epoch: 931, Loss: 3.422\n",
      "Epoch: 932, Loss: 3.613\n",
      "Epoch: 933, Loss: 3.714\n",
      "Epoch: 934, Loss: 3.582\n",
      "Epoch: 935, Loss: 3.890\n",
      "Epoch: 936, Loss: 3.932\n",
      "Epoch: 937, Loss: 3.589\n",
      "Epoch: 938, Loss: 4.097\n",
      "Epoch: 939, Loss: 3.476\n",
      "Epoch: 940, Loss: 3.883\n",
      "Epoch: 941, Loss: 3.876\n",
      "Epoch: 942, Loss: 3.707\n",
      "Epoch: 943, Loss: 4.154\n",
      "Epoch: 944, Loss: 3.713\n",
      "Epoch: 945, Loss: 4.148\n",
      "Epoch: 946, Loss: 3.276\n",
      "Epoch: 947, Loss: 3.489\n",
      "Epoch: 948, Loss: 3.840\n",
      "Epoch: 949, Loss: 4.087\n",
      "Epoch: 950, Loss: 3.042\n",
      "Epoch: 951, Loss: 4.074\n",
      "Epoch: 952, Loss: 3.802\n",
      "Epoch: 953, Loss: 3.395\n",
      "Epoch: 954, Loss: 3.491\n",
      "Epoch: 955, Loss: 3.330\n",
      "Epoch: 956, Loss: 4.621\n",
      "Epoch: 957, Loss: 3.904\n",
      "Epoch: 958, Loss: 3.311\n",
      "Epoch: 959, Loss: 3.529\n",
      "Epoch: 960, Loss: 3.675\n",
      "Epoch: 961, Loss: 3.579\n",
      "Epoch: 962, Loss: 3.807\n",
      "Epoch: 963, Loss: 4.478\n",
      "Epoch: 964, Loss: 3.772\n",
      "Epoch: 965, Loss: 3.786\n",
      "Epoch: 966, Loss: 4.211\n",
      "Epoch: 967, Loss: 4.240\n",
      "Epoch: 968, Loss: 4.030\n",
      "Epoch: 969, Loss: 3.078\n",
      "Epoch: 970, Loss: 3.266\n",
      "Epoch: 971, Loss: 4.221\n",
      "Epoch: 972, Loss: 4.122\n",
      "Epoch: 973, Loss: 3.520\n",
      "Epoch: 974, Loss: 3.984\n",
      "Epoch: 975, Loss: 4.352\n",
      "Epoch: 976, Loss: 3.473\n",
      "Epoch: 977, Loss: 3.385\n",
      "Epoch: 978, Loss: 4.526\n",
      "Epoch: 979, Loss: 4.001\n",
      "Epoch: 980, Loss: 3.916\n",
      "Epoch: 981, Loss: 3.914\n",
      "Epoch: 982, Loss: 3.573\n",
      "Epoch: 983, Loss: 3.537\n",
      "Epoch: 984, Loss: 4.303\n",
      "Epoch: 985, Loss: 3.603\n",
      "Epoch: 986, Loss: 4.171\n",
      "Epoch: 987, Loss: 3.948\n",
      "Epoch: 988, Loss: 3.432\n",
      "Epoch: 989, Loss: 3.133\n",
      "Epoch: 990, Loss: 3.095\n",
      "Epoch: 991, Loss: 3.784\n",
      "Epoch: 992, Loss: 4.044\n",
      "Epoch: 993, Loss: 3.447\n",
      "Epoch: 994, Loss: 3.232\n",
      "Epoch: 995, Loss: 3.523\n",
      "Epoch: 996, Loss: 4.071\n",
      "Epoch: 997, Loss: 3.388\n",
      "Epoch: 998, Loss: 4.385\n",
      "Epoch: 999, Loss: 3.630\n",
      "Epoch: 1000, Loss: 4.193\n"
     ]
    }
   ],
   "source": [
    "# Now load the boston data from sklearn\n",
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 1000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        sgd_update(trainables)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
